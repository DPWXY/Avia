{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"your_openai_api_key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/Input_Response_Flight_Scenarios.json', 'r') as file:\n",
    "    data1 = json.load(file)\n",
    "    \n",
    "with open('data/Input_Response_Turn_2.json', 'r') as file:\n",
    "    data2 = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_training_data(data):\n",
    "    formatted_data = []\n",
    "    \n",
    "    for entry in data:\n",
    "        input_blocks = []\n",
    "        \n",
    "        for point in entry[\"input\"]:\n",
    "            block = \"\\n\".join([f\"{key}: {value}\" for key, value in point.items()])\n",
    "            input_blocks.append(block)\n",
    "        \n",
    "        formatted_input = \"\\n\\n\".join(input_blocks)\n",
    "        response = entry[\"response\"]\n",
    "        \n",
    "        formatted_data.append({\"input\": formatted_input, \"response\": response})\n",
    "    \n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "formate_data1 = format_training_data(data1)\n",
    "formate_data2 = format_training_data(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"data/train_data_flight_turn_v2.jsonl\"\n",
    "with open(train_file, 'w') as jsonl_file:\n",
    "    for dictionary in formate_data1 + formate_data2:\n",
    "        message = {\"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"Evaluate the flight turn data\"},\n",
    "            {\"role\": \"user\", \"content\": dictionary[\"input\"]},\n",
    "            {\"role\": \"assistant\", \"content\": dictionary[\"response\"]}\n",
    "            ]}\n",
    "        jsonl_file.write(json.dumps(message) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 100\n",
      "First example:\n",
      "{'role': 'system', 'content': 'Evaluate the flight turn data'}\n",
      "{'role': 'user', 'content': 'Point: 0\\nASI: 93\\nAI: level\\nHeading: 180\\nTurn and Slip: turn needle right\\nBank Angle: 50\\nVSI: 50\\n\\nPoint: 36\\nASI: 85\\nAI: level\\nHeading: 90\\nTurn and Slip: turn needle right\\nBank Angle: 40\\nVSI: 100\\n\\nPoint: 72\\nASI: 93\\nAI: level\\nHeading: 180\\nTurn and Slip: turn needle left\\nBank Angle: 40\\nVSI: 100\\n\\nPoint: 108\\nASI: 85\\nAI: level\\nHeading: 90\\nTurn and Slip: turn needle left\\nBank Angle: 40\\nVSI: -100\\n\\nPoint: 144\\nASI: 93\\nAI: level\\nHeading: 0\\nTurn and Slip: turn needle right\\nBank Angle: 30\\nVSI: -100\\n\\nPoint: 180\\nASI: 87\\nAI: level\\nHeading: 270\\nTurn and Slip: turn needle right\\nBank Angle: 40\\nVSI: 100\\n\\nPoint: 216\\nASI: 93\\nAI: level\\nHeading: 90\\nTurn and Slip: turn needle left\\nBank Angle: 50\\nVSI: 50\\n\\nPoint: 252\\nASI: 100\\nAI: level\\nHeading: 90\\nTurn and Slip: turn needle right\\nBank Angle: 60\\nVSI: -50\\n\\nPoint: 288\\nASI: 100\\nAI: level\\nHeading: 0\\nTurn and Slip: turn needle left\\nBank Angle: 50\\nVSI: 100\\n\\nPoint: 324\\nASI: 95\\nAI: level\\nHeading: 90\\nTurn and Slip: turn needle right\\nBank Angle: 40\\nVSI: -50'}\n",
      "{'role': 'assistant', 'content': 'Aim for a bank angle of 45 degrees, currently at 40 degrees. Maintain level flight; your vertical speed was -50 feet per minute. Ensure the turn and slip indicator is centered.'}\n",
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "## Functions from OpenAi for checking the training data and calculate the tokens\n",
    "import json\n",
    "import tiktoken # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "data_path = \"data/train_data_flight_turn_v2.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)\n",
    "    \n",
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune OpenAi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-4U8dnyLjfViGUqTrrBRo1K', bytes=141775, created_at=1741898889, filename='train_data_flight_turn_v2.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"your_openai_api_key\")\n",
    "# Upload the training file to openai\n",
    "client.files.create(\n",
    "  file=open(\"data/train_data_flight_turn_v2.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncPage[FileObject](data=[FileObject(id='file-4U8dnyLjfViGUqTrrBRo1K', bytes=141775, created_at=1741898889, filename='train_data_flight_turn_v2.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None), FileObject(id='file-LJBTJx7y7pushfPM5AFiYc', bytes=135375, created_at=1741898759, filename='train_data_flight_turn.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None), FileObject(id='file-HK7wpbX1p1pQLfYB1BPW86', bytes=26492, created_at=1741742335, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None, expires_at=None), FileObject(id='file-8Rdf6T269rdX4F8Kp2Pxvj', bytes=26544, created_at=1741739996, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None, expires_at=None), FileObject(id='file-1Vp5noGdywWp1nD4U3enuY', bytes=176629, created_at=1741736976, filename='train_data_all-prompt-response.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None), FileObject(id='file-5DjaEaSgjbhSiUxZf2Rkrh', bytes=9600, created_at=1740513413, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None, expires_at=None), FileObject(id='file-JEQcLpuuw4mgZtuXPDm7dY', bytes=73093, created_at=1740512571, filename='train_data_one-prompt-response.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None), FileObject(id='file-2xo2JhG1jiLt44PYw3JhWw', bytes=19404, created_at=1738869788, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None, expires_at=None), FileObject(id='file-DLgqcpU6cJ9zNsKiF5BQAy', bytes=122047, created_at=1738868724, filename='train_data_v2.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None), FileObject(id='file-VPWdGSuGzHMK9GKsbjRhAh', bytes=126235, created_at=1738868595, filename='train_data_v2.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None), FileObject(id='file-3zGUuCfjqFTpZXYbMVita1', bytes=109166, created_at=1738868231, filename='train_data_v2.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None), FileObject(id='file-C5VfU1Tnqw2NwZVFNryF93', bytes=73093, created_at=1738217491, filename='train_data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None), FileObject(id='file-HYpvjct7x2Qn5iwVEdSR8Q', bytes=9688, created_at=1738106575, filename='step_metrics.csv', object='file', purpose='fine-tune-results', status='processed', status_details=None, expires_at=None), FileObject(id='file-1Apywjw5uVNhtw9VxNtmCd', bytes=73093, created_at=1738105795, filename='train_data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None), FileObject(id='file-6vwhW2ejUBZ8hrz7wsebAi', bytes=54345, created_at=1738091352, filename='train_data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None, expires_at=None)], object='list', has_more=False, first_id='file-4U8dnyLjfViGUqTrrBRo1K', last_id='file-6vwhW2ejUBZ8hrz7wsebAi')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check all uploaded files\n",
    "client.files.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-1I9psPmZJWq1VxWbbfkMHuOl', created_at=1741898898, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id='org-RO3Int5VH4kDD0ib79PXZngn', result_files=[], seed=1071881758, status='validating_files', trained_tokens=None, training_file='file-4U8dnyLjfViGUqTrrBRo1K', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None, metadata=None, method={'type': 'supervised', 'supervised': {'hyperparameters': {'batch_size': 'auto', 'learning_rate_multiplier': 'auto', 'n_epochs': 'auto'}}})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create fine-tuning job\n",
    "client.fine_tuning.jobs.create(\n",
    "    training_file=\"file-4U8dnyLjfViGUqTrrBRo1K\",\n",
    "    model=\"gpt-4o-2024-08-06\" # gpt-4o-mini-2024-07-18 or gpt-4o-2024-08-06\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-GYK2mL6TZuBcfCntIhjW10T1', created_at=1741898769, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id='org-RO3Int5VH4kDD0ib79PXZngn', result_files=[], seed=707743265, status='queued', trained_tokens=None, training_file='file-LJBTJx7y7pushfPM5AFiYc', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None, metadata=None, method={'type': 'supervised', 'supervised': {'hyperparameters': {'n_epochs': 3, 'batch_size': 1, 'learning_rate_multiplier': 2.0}}})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check fine-tuning job\n",
    "client.fine_tuning.jobs.list()\n",
    "# or \n",
    "client.fine_tuning.jobs.retrieve(\"ftjob-GYK2mL6TZuBcfCntIhjW10T1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[FineTuningJobEvent](data=[FineTuningJobEvent(id='ftevent-tuOm68cxGkLqw9AwAAMwCJTY', created_at=1741899903, level='info', message='Step 116/300: training loss=0.28', object='fine_tuning.job.event', data={'step': 116, 'train_loss': 0.2811059057712555, 'total_steps': 300, 'train_mean_token_accuracy': 0.95652174949646}, type='metrics')], object='list', has_more=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.list_events(\n",
    "  fine_tuning_job_id=\"ftjob-1I9psPmZJWq1VxWbbfkMHuOl\",\n",
    "  limit=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
